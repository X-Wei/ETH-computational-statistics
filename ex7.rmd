---
title: "ex7: LDA/QDA on iris"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

The iris dataset is in R, just use it: 

```{r}
summary(iris)
Iris <- iris[,c("Petal.Length", "Petal.Width", "Species")]
grIris <- as.integer(Iris[,"Species"])
set.seed(16)
# install.packages("MASS")
library(MASS)
n <- nrow(Iris)
```

## a) fit with LDA/QDA, plot clf boundaries
The plot function is given in the skeleton: 
```{r}
## Utility functiom for plotting boundaries
predplot <- function(object, x, gr = grIris, main = "", lines.only=FALSE,
                     len = 42, colcont = "black", ...)
{
  ##  gr : the true grouping/class vector
  stopifnot(length(gr) == nrow(x))
  xp <- seq(min(x[,1]), max(x[,1]), length=len)
  yp <- seq(min(x[,2]), max(x[,2]), length=len)
  grid <- expand.grid(xp, yp)
  colnames(grid) <- colnames(x)[-3]
  Z <- predict(object, grid, ...)
  zp <- as.numeric(Z$class)
  zp <- Z$post[,3] - pmax(Z$post[,2], Z$post[,1])
  if(!lines.only)
    plot(x[,1], x[,2], col =gr, pch = gr,
         main = main,xlab=colnames(x)[1],ylab=colnames(x)[2])
  contour(xp, yp, matrix(zp, len),
          add = TRUE, levels = 0, drawlabels = FALSE, col=colcont)
  zp <- Z$post[,1] - pmax(Z$post[,2], Z$post[,3])
  contour(xp, yp, matrix(zp, len),
          add = TRUE, levels = 0, drawlabels = FALSE, col=colcont)
}
```

First have a look at documentation for lda and qda in the pacakge MASS. Then fill the skeleton: 

```{r}
## Use function lda to fit data
class_lda <- lda(Species~Petal.Length+Petal.Width, data=Iris)
## Use function predplot to plot the boundaries
predplot(class_lda, Iris, main="Classification with LDA")

## Use function qda to fit data
class_qda <- qda(Species~Petal.Length+Petal.Width, data=Iris)
## Use function predplot to plot the boundaries
predplot(class_qda, Iris, main="Classification with QDA")
```


## b) fit Bootstrap samples
Plot the bootstrap estimates mu_hat(j, i) j=1:3 i=1:B in a single plot. 

first fit to each bs samples: 
```{r}
## Bootstrap size
B <- 1000
## Create a random index matrix with either functions sample or sample.int to generate bootstrap
index <- matrix( sample(1:n, size=n*B, replace=T), nrow=n, ncol=B ) # each col is a bs sample
## Initialize the list for LDA nad QDA fits
fit_lda <- vector("list",B)
fit_qda <- vector("list",B)

## Use both methods on the bootstrap samples
for(i in 1:B) {
  ind <- index[,i] # ith BS sample
  fit_lda[[i]] <- lda(Species~Petal.Length+Petal.Width, data=Iris[ind,])
  fit_qda[[i]] <- qda(Species~Petal.Length+Petal.Width, data=Iris[ind,])
}
```

determine mu_hat: 
```{r}
## Initialize the mu_hat bootstrap estimates (only for lda)
mu_hat_1 <- mu_hat_2 <- mu_hat_3 <- matrix(0,ncol=B,nrow=2) # for each mu(j,i), 2 estimates (lad/qda)
## Determine the mu_hat bootstrap estimates
for(i in 1:B){
  mu_hat_temp <- fit_lda[[i]]$means
  mu_hat_1[,i] <- mu_hat_temp[1,]
  mu_hat_2[,i] <- mu_hat_temp[2,]
  mu_hat_3[,i] <- mu_hat_temp[3,]
}
```

To plot the means, we need to add points to current plot using `points`, and we should first calculate the appropriate xlim/ylim using `extendrange`: 
```{r}
## Plot the boostrapped estimators
all_mu <- cbind(mu_hat_1, mu_hat_2, mu_hat_3)
xlim <- extendrange(all_mu[1,])
ylim <- extendrange(all_mu[2,])

plot(mu_hat_1[1,], mu_hat_1[2,], col=1, pch=1,
     xlim=xlim, ylim=ylim,
     xlab="Petal Length", ylab="Petal Width", main="Bootstrap estimated means")
points(mu_hat_2[1,], mu_hat_2[2,], col=2, pch=2)
points(mu_hat_3[1,], mu_hat_3[2,], col=3, pch=3)
```

## c) plot clf boundary for bootstraps (both lda and qda)
For each bs sample, plot the boundary ==> many boundaris in the same plot ==> use the `lines.only` param in the provided predplot function. Also can use transparent color for the bs boundaries, to generate a transparent color, use `adjustcolor`: 

```{r}
## Plot the bootstrapped boundaries estimates with LDA
predplot(class_lda, Iris,
         main = "Bootstrapped boundaries estimates with LDA")
for(i in 1:B){
  fit <- fit_lda[[i]]
  predplot(fit, Iris, lines.only= TRUE, colcont=adjustcolor("gray", 0.25))
}



## Plot the bootstrapped boundaries estimates with QDA
predplot(class_qda, Iris,
         main= "Bootstrapped boundaries estimates with QDA")
for(i in 1:B){
  fit <- fit_qda[[i]]
  predplot(fit, Iris, lines.only= TRUE, colcont=adjustcolor("gray", 0.25))
}
```

## d) boostrap estimate of the generalization error

Read the doc for `predict.lda`. 

```{r}
## Initialize the errors

error_lda <- rep(0,B)
error_qda <- rep(0,B)

## Use the predict function to calculate the error
## Read help on predict.lda. Remember that logical
## FALSE/TRUE are treated as 0/1.

for(i in 1:B){
  fit_temp1 <- predict(fit_lda[[i]], newdata = Iris)$class
  error_lda[i] <- sum(fit_temp1!=Iris$Species)
  fit_temp2 <- predict(fit_qda[[i]], newdata = Iris)$class
  error_qda[i] <- sum(fit_temp2!=Iris$Species)
}

## Print the error
cat("Generalized error for LDA:",format(mean(error_lda),digits=4))
cat("Generalized error for QDA:",format(mean(error_qda),digits=4))


## Plot the boxplot of the errors
boxplot(cbind( error_lda, error_qda ), notch=T, main="bootstrap generalization error")
```


## e) OOB estimates

predict on the unseen data: just use `newdata=Iris[-ind,]` in predict: 

```{r}
## Initialize the errors

ooberr_lda <- rep(0,B)
ooberr_qda <- rep(0,B)

## Use the predict function to calculate the error
## Read help on predict.lda. Remember that logical
## FALSE/TRUE are treated as 0/1.

for(i in 1:B){
  ind <- index[,i] # ith BS sample
  unseen <- Iris[-ind,]
  fit_temp1 <- predict(fit_lda[[i]], newdata = unseen)$class
  ooberr_lda[i] <- sum(fit_temp1!=unseen$Species)
  fit_temp2 <- predict(fit_qda[[i]], newdata = unseen)$class
  ooberr_qda[i] <- sum(fit_temp2!=unseen$Species)
}

## Print the error
cat("OOB Generalized error for LDA:",format(mean(ooberr_lda),digits=4))
cat("OOB Generalized error for QDA:",format(mean(ooberr_qda),digits=4))


## Plot the boxplot of the errors
boxplot(cbind( error_lda, error_qda, ooberr_lda, ooberr_qda ), notch=T, main="OOB bootstrap generalization error")
```

Remarks: 
>The OOB estimates are calculated using data not included in the bootstrap sample, so the resulting error has a bigger variance and a smaller bias as compared to the ordinary estimate of the generalization error, as is evident from the boxplot. 
>From the (ordinary) estimate of generr, we conclude QDA is better, but form the OOB boxplot, the difference is not as dramatic. But the QDA is still better, and has smaller variance. So we conclude QDA is better. 


